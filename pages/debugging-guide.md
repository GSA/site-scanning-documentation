The following is a step by step means to looking for where bad data might come into the process and be incorporated in the final site scanning data files. 

_(To add: directions for running scans using the dev environment; how to track the progress and completion of the above steps.)_


## Analysis 

* Review [the original datasets themselves](https://github.com/GSA/federal-website-index/blob/main/builder/config.py) that are combined in order to generate the target URL list.  One can look at the original source files or at [snapshots of them](https://github.com/GSA/federal-website-index/tree/main/data/snapshots) which were generated the last time that the target URL list was updated, in case the source files have changed in the interim.  
* Think through the [formal order and logic](https://github.com/GSA/federal-website-index/blob/main/builder/main.py) which which the original datasets are then combined, dedupped, and trimmed in order to form the target URL list.  
  * Then, look at [the snapshot files](https://github.com/GSA/federal-website-index/tree/main/data/snapshots) which were generated at each step during the last build in order to look for the introduction of errors.  [This analysis file is also then generated during the target URL list creation process](https://github.com/GSA/federal-website-index/blob/main/data/site-scanning-target-url-list-analysis.csv) that contains useful numbers.  
* At this point, look at the assembled [target URL list](https://github.com/GSA/federal-website-index/blob/main/data/site-scanning-target-url-list.csv).  This is the file that the Site Scanning program ingests to build its list of urls to scan.  [This analysis file](https://github.com/GSA/site-scanning-analysis/blob/main/reports/target-URL-list.csv) looks at the static, completed target URL list.
* The `primary` (live only) and `all` snapshots of the Site Scanning results can be found [here](https://api.gsa.gov/technology/site-scanning/data/weekly-snapshot.csv) and [here](https://api.gsa.gov/technology/site-scanning/data/weekly-snapshot-all.csv), respectively.  
  * Analysis files for the primary and all snapshots can be found [here](https://github.com/GSA/site-scanning-analysis/blob/main/reports/snapshot-primary.csv) and [here](https://github.com/GSA/site-scanning-analysis/blob/main/reports/snapshot-all.csv), respectively.  

## Rebuilding

The regular, automated schedule by which the target URL list is assembled and ingested, that the scans are run, and that the analysis files and shapshots are generated can be found [here](https://github.com/GSA/site-scanning-documentation/blob/main/pages/schedule.md).  They will all take place on their own each week, but each step can also be manually triggered to help with debugging.  

Here is the order to consider for each step of the process if one wants to rebuild the any or all of these files:  

1. Make any edits to the original source datasets (links [here](https://github.com/GSA/federal-website-index/blob/main/builder/config.py)) that will be combined in order to generate the target URL list.  
2. Run the `build target URL list` [action](https://github.com/GSA/federal-website-index/actions).  This re-assembles the target URL list.  The [analysis file](https://github.com/GSA/federal-website-index/blob/main/data/site-scanning-target-url-list-analysis.csv) for the target URL list creation process should be updated at the same time.  The analysis file that looks at the static, completed target URL list could also then be regenerated by running the `Generate target URL list analysis` action [here](https://github.com/GSA/site-scanning-analysis/actions).  
3. Once the target URL list has been regenerated, the next step would be to re-ingest that file into the Site Scanning engine by running the `Ingest` action [here](https://github.com/GSA/site-scanning-engine/actions).  
4. Once the ingesting process has completed, the way to kick off scans afresh is to run the  `Enqueue` action [here](https://github.com/GSA/site-scanning-engine/actions).  
5. After the scans have completed, generate the snapshots afresh by running the `Create S3 snapshot` action [here](https://github.com/GSA/site-scanning-engine/actions).  
6. After the snapshots have been created, regenerate the analysis files by running the `Generate all analysis` action [here](https://github.com/GSA/site-scanning-analysis/actions).  


## Overall Quality Assurance Process

* Look at [Federal Website Index](https://github.com/GSA/federal-website-index/blob/main/data/site-scanning-target-url-list.csv) 
  * Look at [two](https://github.com/GSA/federal-website-index/blob/main/data/site-scanning-target-url-list-analysis.csv) [reports](https://github.com/GSA/site-scanning-analysis/blob/main/reports/target-url-list.csv) for oddities 
  * Directly note number of target URLs
  * Check if any are duplicative
  * search for .com, .edu, .org
    
