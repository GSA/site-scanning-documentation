One way to understand what [data the Site Scanning program offers](https://digital.gov/guides/site-scanning/understand-the-data/) is to look at what questions the data answers...  

* How many public, federal .gov websites are there? 
* How many public .gov websites does agency X (e.g. the Justice Department) operate?  
* How many public websites does Y domain (e.g. state.gov) operate? 
* Which agency websites redirect and to where?
* For any given website or group of websites - how many have implemented the US Web Design System?  Which have and which haven't?  
* For any given website or group of websites, which parts of the US Web Design System have they implemented and which parts have they not?  Which version of it are they using?  
* For any given website or group of websites - how many have implemented the Digital Analytics Program?  Which have and which haven't?  
* For any given website or group of websites, how have they customized their implementation of the Digital Analytics Program?  
* For any given website or group of websites - how many have implemented search engine optimization?  Which have and which haven't?  
* For any given website or group of websites, which aspects of search engine optimization have they implemented and which parts have they not?  Which version of it are they using?  
* For any given website, what are particular details of their sitemaps that the Search.gov team needs to know.  
* For any given website or group of websites, how many and which third party services do they use? 
* For any given website or group of websites, how many are IPv6 compatible?  Which are and which aren't? 



